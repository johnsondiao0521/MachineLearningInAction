# 机器学习总结

## 机器学习概述
人工智能之父：图灵
![人工智能发展历史](https://i.imgur.com/DgtmBFc.png)

列举应用场景：
1. 南方都市报的“小南”，广州日报的“阿同”机器人
2. 识别车的品牌类型
3. 医疗癌症的识别
4. 艺术画
5. iPhone X
6. 推荐系统

各类框架的应用如图：

![机器学习框架介绍](https://i.imgur.com/XDgBVI3.png)

* 什么是机器学习

机器学习是从**数据**中自动分析获得**规律（模型）**，并利用规律对**未知数据进行预测**

比如：AlphaGo，搜狗的广告推荐

* 为什么需要机器学习
![为什么需要机器学习](https://i.imgur.com/PdxX2l7.png)

* 机器学习的应用场景
1. 自然语言处理
2. 无人驾驶
3. 计算机视觉
4. 推荐系统



## 数据的特征工程

* 数据来源
1. 企业日益积累的大量数据（互联网公司更为显著）
2. 政府掌握的各种数据
3. 科研机构的实验数据
......

* 数据类型

 - 离散型数据：由记录不同类别个体的数目所得到的数据，又称计数数据，所有这些数据全部都是整数，而且不能再细分，也不能进一步提高他们的精确度。

 - 连续型数据：变量可以在某个范围内取任一数，即变量的取值可以是连续的，如，长度、时间、质量值等，这类整数通常是非整数，含有小数部分。

 - 注：只要记住一点，离散型是区间内不可分，连续型是区间内可分。

* 可用的数据集
 - scikit-learn特点：
	1. 数据量较小 
	2. 方便学习

 - UCI特点：
	1. 收录了360个数据集 
	2. 覆盖科学、生活、经济等领域           
	3. 数据量几十万

 - Kaggle特点：
	1. 大数据竞赛平台
    2. 80万科学家
    3. 真实数据
    4. 数据量巨大

* 常用数据集数据的结构组成
 - 结构：特征值+目标值

## 数据的特征工程

1. 特征工程是什么

 - 特征工程是将**原始数据转换为更好地代表预测模型的潜在问题的特征**的过程，从而**提高了对未知数据的模型准确性**

2. 特征工程的意义
 - 直接影响模型的预测结果

3. scikit-learn库介绍
  - Python语言的机器学习工具
  - Scikit-learn包括许多知名的机器学习算法的实现
  - Scikit-learn文档完善，容易上手，丰富的API，使其在学术界颇受欢迎。
  - 目前稳定版本0.18

4. 数据的特征抽取
 - 特征抽取实例演示
 	-  通过演示得出结论：
		 1. 特征抽取针对非连续型数据 
		 2. 特征抽取对文本等进行特征值化。
 - sklearn特征抽取API
	- sklearn.feature_extraction
 - 字典特征抽取
	- 作用：对字典数据进行特征值化
	- 类：sklearn.feature_extraction.DictVectorizer
 - DictVectorizer语法,DictVectorizer(sparse=True,…)
	- DictVectorizer.fit_transform(X)       
		1. X:字典或者包含字典的迭代器
		2. 返回值：返回sparse矩阵
	- DictVectorizer.inverse_transform(X)
		1. X:array数组或者sparse矩阵
		2. 返回值:转换之前数据格式
	- DictVectorizer.get_feature_names()
		1. 返回类别名称
	- DictVectorizer.transform(X)
		1. 按照原先的标准转换

``` 引入类库
from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler,StandardScaler,Imputer
from sklearn.feature_selection import VarianceThreshold
from sklearn.decomposition import PCA
import jieba
import numpy as np
```

``` 字典特征化实例
	def dictVector():
    '''
        对字典进行特征化
    :return:
    '''
    dict=DictVectorizer(sparse=False)
    data=dict.fit_transform([{'city': '北京','temperature':100},{'city': '上海','temperature':60},{'city': '深圳','temperature':30}])
    print(dict.get_feature_names())
    print(data)
    return None
```

 - 文本特征抽取
	- 作用：对**文本数据**进行特征值化
	- sklearn.feature_extraction
 - CountVectorizer语法
	- CountVectorizer(max_df=1.0,min_df=1,...)
		1. 返回词频矩阵
	- CountVectorizer.fit_transform(X,y)
		1. X:文本或者包含文本字符串的可迭代对象
		2. 返回值：返回sparse矩阵
	- CountVectorizer.inverse_transform(X)
		1. X:array数组或者sparse矩阵
		2. 返回值：转换之前数据格式
	- CountVectorizer.get_feature_names()
		1. 返回值：单词列表

```
def countVector():
    '''
        对文本进行特征化
    :return:
    '''
    count=CountVectorizer()
    data=count.fit_transform(["life is is short short,i like python","life is too long,i dislike python"])
    print(count.get_feature_names())
    print(data.toarray())
    return None
```

5. 数据的特征处理

6. 数据的特征选择

7. 降维



## 文本中文特征话的解决
* 如何对中文文本特征值化
* jieba分词
* 案例

```
1、今天很残酷，明天更残酷，后天很美好，
但绝对大部分是死在明天晚上，所以每个人不要放弃今天。

2、我们看到的从很远星系来的光是在几百万年之前发出的，
这样当我们看到宇宙时，我们是在看它的过去。

3、如果只用一种方式了解某样事物，你就不会真正了解它。
了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。

```
* 案例：对三段话进行特征值化——流程
 1. 准备句子，利用jieba.cut进行分词
 2. 实例化CountVectorizer
 3. 将分词结果变成字符串当作fit_transform的输入值

```
def cutWord():
    con1=jieba.cut("今天很残酷，明天更残酷，后天很美好，但绝大部分是死在明天晚上，所以每个人不要放弃今天")
    con2=jieba.cut("我们看到的从很远星系来的光是从几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。")
    con3=jieba.cut("如果只用一种方式了解某样事物，你就不会真正了解它，了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。")
    #转换成列表
    content1=list(con1)
    content2=list(con2)
    content3=list(con3)
    #把列表转换成字符串
    c1=" ".join(content1)
    c2=" ".join(content2)
    c3=" ".join(content3)
    return c1,c2,c3

def hanziVector():
    '''
        中文特征值化
    :return:
    '''
    c1,c2,c3=cutWord()

    count=CountVectorizer()
    data=count.fit_transform([c1,c2,c3])
    print(count.get_feature_names())
    print(data.toarray())
    return None
```

* 词语占比

下图所展示的是词语在文章中的占比，可以大概确定文章所属的类型
![文章类型](https://i.imgur.com/6goF39s.png)

* 引入TF-IDF
 - TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。
 - TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的**重要程度**。
 - 类：sklearn.feature_extraction.text.TfidfVectorizer
 - 为什么需要TfidfVectorizer
	- 分类机器学习算法的重要依据
* 备注
 - Tf:term frequency 词的频率
 - idf:逆文档频率 inverse document frequency log(总文档数量/该词出现的文档数量)
 - log(数值) 输入的数值越小，结果越小
 - tf*idf 重要性程度

```
def tfidfVector():
    '''
		tfidf 特征值化
    :return:
    '''
    c1,c2,c3=cutWord()
    df=TfidfVectorizer()
    data=df.fit_transform([c1,c2,c3])
    print(df.get_feature_names())
    print(data.toarray())
    return None
```

## 数据的特征预处理

### 特征处理的方法
* 通过特定的统计方法（数学方法）将数据转换成算法要求的数据

| 特征1 | 特征2 | 特征3 | 特征4 |
| ------: | ------: | ------: | ------: |
| 90 | 2 | 10 | 40 |
| 60 | 4 | 15 | 45 |
| 75 | 3 | 13 | 46 |

![数组缩放](https://i.imgur.com/iauoM1a.png)

* 数值型数据
 - 标准缩放
	1. 归一化
	2. 标准化
	3. 缺失值
 - 类别型数据：one-hot 编码
 - 时间类型：时间的切分

### sklearn特征处理API
#### sklearn.preprocessing
* 归一化
 - 特点：通过对原始数据进行变换把数据映射到(默认为[0,1])之间
 - 公式：X'=x-min/max-min,X``=X`*(mx-mi)+mi
 - 注：作用于每一列，max为一列的最大值，min为一列的最小值，那么X``为最终结果，mx，mi分别为指定区间值默认mx为1，mi为0

![归一化公式计算过程](https://i.imgur.com/5uQyTbL.png)
* sklearn归一化API
 - sklearn归一化API:  sklearn.preprocessing.MinMaxScaler
 - MinMaxScaler语法
 	- MinMaxScaler(feature_range(0,1)...)
	  - 每个特征缩放到给定范围(默认【0,1】)
	  - MinMaxScaler.fit_transform(X)
		- X:numpy array格式的数据[n_samples,n_features]
		- 返回值：转换后的形状相同的array

```
def minMax():
    '''
        归一化处理
    :return:
    '''
    mm=MinMaxScaler(feature_range=(0,1))
    data=mm.fit_transform([[90,2,10,40],[60,4,15,45],[75,3,13,46]])
    print(data)
    return None
```
 - 归一化案例：约会对象数据
	- 相亲约会对象数据，这个样本时男士的数据，三个特征，玩游戏所消耗时间的
百分比、每年获得的飞行常客里程数、每周消费的冰淇淋公升数。然后有一个 
所属类别，被女士评价的三个类别，不喜欢didnt、魅力一般small、极具魅力large
也许也就是说飞行里程数对于结算结果或者说相亲结果影响较大，但是统计的
人觉得这三个特征同等重要。

| 里程数 | 公升数 | 消耗时间比 | 评价 |
| ------: | ------: | ------: | ------: |
| 14488 | | 7.153469 | 1.673904 | smallDoses |
| 26052 | | 1.441871 | | 0.805124 | | didntLike |
| 75136 | | 13.147394 | |0.428964 | | didntLike |
| 38344 | | 1.669788 | | 0.134296 | | didntLike |
| 72993 | | 10.141740 | | 1.032955 | | didntLike |
| 35948 | | 6.830792 | | 1.213192 | | largeDoses |
| 42666 | | 13.276369 | | 0.543880 | | largeDoses |
| 67497 | | 8.631577 | | 0.749278 | | didntLike |
| 35483 | | 12.273169 | | 1.508053 | | largeDoses |
| 50242 | | 3.723498 | | 0.831917 | | didntLike |

 - 问题：如果数据中异常点较多，会有什么影响？
 - 归一化总结
	- 注意在特定场景下最大值最小值是变化的，另外，最大值与最小值非常容易受**异常**点影响，所以这种方法鲁棒性较差，只适合**传统精确小数据场景**。


* 标准化
 - 特点：通过对原始数据进行变换把数据变换到均值为0,方差为1范围内
 - 公式：X`=x-mean/σ  
 - 注：作用于每一列，mean为平均值，σ为标准差（考量数据的稳定性）
 - std成为方差，std=(x1-mean)∧2 + (x2-mean)∧2 + .../n(每个特征的样本数)，σ=√std
* 结合归一化来谈标准化
 - 对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变。
 - 对于标准化来说，如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。
 - sklearn特征化API:  scikit-learn.preprocessing.StandardScaler
 - StandardScaler语法
 	- StandardScaler(…) 
	  - 处理之后每列来说所有数据都聚集在均值0附近方差为1
	  - StandardScaler.fit_transform(X,y)       
		- X:numpy array格式的数据[n_samples,n_features]
		- 返回值：转换后的形状相同的array
	  - StandardScaler.mean_
		- 原始数据中每列特征的平均值
	  - StandardScaler.std_
		- 原始数据每列特征的方差
 - 标准化总结
 	- 在已有**样本足够多的情况下比较稳定**，适合现代嘈杂大数据场景

```
def standard():
    '''
        标准化缩放
    :return:
    '''
    standard=StandardScaler()
    data=standard.fit_transform([[1.,-1.,3.],[2.,4.,2.],[4.,6.,-1.]])
    print(data)
    return None
```

* 缺失值
 - 如何处理数据中的缺失值？
	- 删除：如果每列或者行数据缺失值达到一定的比例，建议放弃整行或整列
	- 插补：可以通过缺失值每列或者每行的平均值、中位数来填充
 - skeanrn缺失值API：sklearn.preprocessing.Imputer
 - Imputer 语法
 	- Imputer(missing_values='Nan',strategy='mean',axis=0)
	  - 完成缺失值插补
	  - Imputer.fit_transform(X,y)       
		- X:numpy array格式的数据[n_samples,n_features]
		- 返回值：转换后的形状相同的array
 - 关于np.nan(np.NaN)
	- numpy的数组中可以使用np.nam/np.NaN来代替缺失值，属于float类型
	- 如果是文件中的一些缺失值，可以替换成nan，通过np.nan转换成float类型的数组即可。

```
def imputer():
    '''
        缺失值处理
    :return:
    '''
    im=Imputer(missing_values="Nan",strategy='mean',axis=0)
    data=im.fit_transform([[1.,2.],[np.nan,3.],[7.,9.]])
    return None
```

## 特征选择
* 特征选择原因
 - 冗余：部分特征的相关度高，容易消耗计算机性能
 - 噪声：部分特征对预测结果有负影响

### 特征选择是什么
* 特征选择就是单纯地从提取到的**所有特征中选择部分特征**作为训练集特征，特征在**选择前和选择后可以改变值、也不改变值**，但是选择后的特征维数肯定比选择前小，毕竟我们只选择了其中的一部分特征。
* 主要方法（三大武器）：
 - Filter(过滤式):VarianceThreshold
 - Embedded(嵌入式)：正则化、决策树
 - Wrapper(包裹式)

### sklearn特征选择API
* sklearn.feature_selection.VarianceThreshold
- VarianceThreshold 语法
 	- VarianceThreshold(threshold = 0.0)
	  - 删除所有低方差特征
	  - Variance.fit_transform(X,y)       
		- X:numpy array格式的数据[n_samples,n_features]
		- 返回值：训练集差异低于threshold的特征将被删除。
		- 默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。

```
def variance():
    '''
        特征选择-删除低方差的特征
    :return:
    '''
    var=VarianceThreshold(threshold=0.0)
    data=var.fit_transform([[0,2,0,3],[0,1,4,3],[0,1,1,3]])
    print(data)
    return None
```

### 其它特征选择方法
* 神经网络

## 降维
* sklearn.decomposition 
* PCA是什么
	- 本质：PCA是一种分析、简化数据集的技术
	- 目的：是数据维数压缩，尽可能降低原数据的维数（复杂度），**损失少量数据**
	- 作用：可以削减回归分析或者聚类分析中特征的数量。
* PCA语法
 - PCA(n_components=None)
 	- 将数据分解为较低维数空间
	- PCA.fit_transform(X)
	  - X:numpy array格式的数据[n_samples,n_features]       
	  - 返回值：转换后指定维度的array

```
def pca():
    '''
        主成分分析进行特征降维
    :return:
    '''
    pca=PCA(n_components=0.9)
    data=pca.fit_transform([[2,8,4,5],[6,3,0,8],[5,4,9,1]])
    print(data)
    return None
```

## 机器学习基础 

### 机器学习开发流程
* （1）算法是核心，数据和计算是基础
* （2）找准定位
	- 大部分复杂模型的算法设计都是算法工程师在做，而我们
		- 分析很多的数据
		- 分析具体的业务
		- 应用常见的算法
		- 特征工程、调参数、优化
* （3）我们应该怎么做
	1. 学会分析问题，使用机器学习算法的目的，想要算法完成何种任务

	2. 掌握算法基本思想，学会对问题用相应的算法解决

	3. 学会利用库或者框架解决问题

* （4）机器学习开发流程
![机器学习开发流程](https://i.imgur.com/n8ZS5du.png)

### 机器学习模型是什么
- 定义：通过一种映射关系将输入值到输出值
![机器学习模型](https://i.imgur.com/u9Cyqyu.png)

### 机器学习算法分类

* 监督学习
	- 分类：k-近邻算法、贝叶斯分类、决策树与随机森林、逻辑回归、神经网络
	- 回归：线性回归、岭回归
	- 标注隐马尔可夫模型(不做要求)
* 无监督学习
	- 聚类：k-means

总结如图所示：

![机器学习算法分类](https://i.imgur.com/PoJaRw2.png)

1. 监督学习（英语：Supervised learning），可以由输入数据中学到或建立一个模型，并依此模式推测新的结果。输入数据是由输入**特征值和目标值**所组成。**函数的输出可以是一个连续的值（称为回归）**，或是**输出是有限个离散值（称作分类）。**

2. 无监督学习（英语：Supervised learning），可以由输入数据中学到或建立一个模型，并依此模式推测新的结果。输入数据是由输入**特征值**所组成。

* 分类问题
	- 概念：分类是监督学习的一个核心问题，在监督学习中，**当输出变量取有限个离散值时，预测问题变成为分类问题。最基础的便是二分类问题**，即判断是非，从两个类别中选择一个作为预测结果；

![分类问题](https://i.imgur.com/bERQB3a.png)
	
* 分类问题的应用
 - 分类在于根据其特性将数据“分门别类”，所以在许多领域都有广泛的应用

 	- 在银行业务中，构建一个客户分类模型，按客户按照贷款风险的大小进行分类

 	- 图像处理中，分类可以用来检测图像中是否有人脸出现，动物类别等

 	- 手写识别中，分类可以用于识别手写的数字

 	- 文本分类，这里的文本可以是新闻报道、网页、电子邮件、学术论文
 	- ...

* 回归问题
 - 概念：回归是监督学习的另一个重要问题。回归用于预测输入变量和输出变量之间的关系，输出是连续型的值。
* 回归问题的应用
 - 房价预测，根据某地历史房价数据，进行一个预测
 - 金融信息，每日股票走向





